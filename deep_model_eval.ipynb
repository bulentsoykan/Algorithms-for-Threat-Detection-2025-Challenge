{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849cd81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from ucf_atd_model.data import data_loc\n",
    "from ucf_atd_model.datasets.create_link_data import calculate_link_features, haversine_distance_m\n",
    "from ucf_atd_model.datasets.create_20class_data import subset, setidx, boolfilter, paddata, const_data\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from ucf_atd_model.datasets.create_link_data import calculate_link_features, haversine_distance_m, project_forward\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "from ucf_atd_model.c20_consts import *\n",
    "\n",
    "import atd2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0d5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = pt.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b972e7",
   "metadata": {},
   "source": [
    "Preprocessing, loading checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f936c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=231, out_features=2000, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Dropout(p=0.1, inplace=False)\n",
       "  (5): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): Linear(in_features=1000, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badnames = [x for x in full_names if x.endswith(\"_16\")]\n",
    "\n",
    "ynames = [x for x in ynames if not x.endswith(\"_16\")]\n",
    "xnames = [x for x in colnames if x not in badnames]\n",
    "\n",
    "# Setup the model\n",
    "inp_dim = len(colnames) - len(badnames) + 1\n",
    "h_dim = 2000\n",
    "out_dim = n_norm_classes + 1 - 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inp_dim, h_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h_dim, h_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(h_dim, h_dim // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    # nn.Linear(h_dim // 2, h_dim // 2),\n",
    "    # nn.ReLU(),\n",
    "    # nn.Dropout(0.2),\n",
    "    nn.Linear(h_dim // 2, out_dim),\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(pt.load(\"checkpoints/epoch_180.pt\", weights_only=True, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fefe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = pt.load(\"xmean.pt\").float()\n",
    "xstd = pt.load(\"xstd.pt\").float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737704e6",
   "metadata": {},
   "source": [
    "Code to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12729456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, model):\n",
    "    \"\"\"Implements the final ML-Enhanced Tracking algorithm.\"\"\"\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    df['track_id'] = -1\n",
    "\n",
    "    next_track_id = 0\n",
    "    \n",
    "    n = df.shape[0]\n",
    "    lastPtInTrack = {\n",
    "        \"time\": np.repeat(pd.Timestamp(year=1970, month=1, day=1, hour=0, minute=0, second=0).to_numpy(), n), \n",
    "        \"lat\": np.repeat(-1.0, n), \n",
    "        \"lon\": np.repeat(-1.0, n), \n",
    "        \"speed\": np.repeat(-1.0, n), \n",
    "        \"course\": np.repeat(-1.0, n),\n",
    "        \"track_id_true\": np.repeat(-1, n)\n",
    "    }\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(df))):\n",
    "        p_current = df.iloc[i]\n",
    "\n",
    "        if next_track_id == 0:\n",
    "            df.loc[i, 'track_id'] = next_track_id\n",
    "            setidx(lastPtInTrack, i, p_current)\n",
    "            next_track_id += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        active_tracks_df = subset(lastPtInTrack, next_track_id)\n",
    "        \n",
    "        time_diff = (p_current[\"time\"].to_numpy() - active_tracks_df[\"time\"]).astype(\"timedelta64[s]\").astype(\"int\")\n",
    "\n",
    "        max_dist_m = time_diff * 30 * 0.5144\n",
    "        real_dist = haversine_distance_m(active_tracks_df[\"lat\"], active_tracks_df[\"lon\"], p_current[\"lat\"], p_current[\"lon\"])\n",
    "        \n",
    "        kinematic_errors = haversine_distance_m(p_current[\"lat\"], p_current[\"lon\"], *project_forward(active_tracks_df['lat'], active_tracks_df['lon'], active_tracks_df['speed'], active_tracks_df['course'], time_diff))\n",
    "        error_cutoff = np.sort(kinematic_errors)[:n_norm_classes].max()\n",
    "        kinematic_filter = kinematic_errors < error_cutoff\n",
    "        \n",
    "        loc_filter = real_dist < max_dist_m\n",
    "        timeCorrect: np.ndarray = (0 < time_diff)\n",
    "        big_filter = loc_filter & timeCorrect & kinematic_filter\n",
    "        idxs = np.arange(len(timeCorrect))\n",
    "\n",
    "        # Create data if we find data points within the filters\n",
    "        if np.any(big_filter):\n",
    "            all_data = paddata(calculate_link_features(boolfilter(active_tracks_df, big_filter), p_current, eval=True))\n",
    "            maindata = all_data[normal_features][:-1].to_numpy()\n",
    "            otherdata = all_data[currpt_features].iloc[0].to_numpy()\n",
    "            \n",
    "            # Predict on this data\n",
    "            toappend = np.zeros((n_norm_classes - 1) * len(normal_features) + len(currpt_features))\n",
    "            raveled = np.ravel(maindata)\n",
    "            toappend[:raveled.shape[0]] = raveled\n",
    "            toappend[raveled.shape[0]:] = otherdata\n",
    "\n",
    "            tensorIn = ((pt.from_numpy(toappend).float() - xmean) / xstd).to(device)\n",
    "\n",
    "            modelOut = model(tensorIn).detach().cpu().numpy()\n",
    "            outMask = np.zeros_like(modelOut)\n",
    "            outMask[:-1] = np.all(maindata == -1, axis=1) * -1e8\n",
    "            outMask[-1] = 0\n",
    "            modelOut = modelOut + outMask\n",
    "            \n",
    "            argMaxModelOut = np.argmax(modelOut)\n",
    "\n",
    "            # Assignment with a confidence threshold\n",
    "            if argMaxModelOut != len(modelOut) - 1:\n",
    "                best_match_track_id = None\n",
    "                try:\n",
    "                    best_match_track_id = idxs[big_filter][argMaxModelOut]\n",
    "                except IndexError:\n",
    "                    print(\"Bad\")\n",
    "                    best_match_track_id = next_track_id\n",
    "                    next_track_id += 1\n",
    "\n",
    "                df.loc[i, 'track_id'] = best_match_track_id\n",
    "                setidx(lastPtInTrack, best_match_track_id, p_current)\n",
    "            else:\n",
    "                df.loc[i, 'track_id'] = next_track_id\n",
    "                setidx(lastPtInTrack, next_track_id, p_current)\n",
    "                next_track_id += 1\n",
    "        else:\n",
    "            df.loc[i, 'track_id'] = next_track_id\n",
    "            setidx(lastPtInTrack, next_track_id, p_current)\n",
    "            next_track_id += 1\n",
    "\n",
    "    return df[['point_id', 'track_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de7baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102861/102861 [07:55<00:00, 216.40it/s]\n"
     ]
    }
   ],
   "source": [
    "truth_df = pd.read_csv(data_loc(\"dataset1_truth.csv\"))\n",
    "\n",
    "truth_df[\"time\"] = pd.to_datetime(truth_df[\"time\"])\n",
    "truth_df[\"time\"] = truth_df[\"time\"].apply(lambda x: datetime.combine(datetime(1970, 1, 1, 0, 0, 0).date(), x.time()))\n",
    "truth_df[\"track_id_true\"] = truth_df[\"track_id\"]\n",
    "\n",
    "output = run_model(truth_df, model)\n",
    "output.to_csv(\"ml_out_ds1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4574d0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5275323008720506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atd2025.accuracy.evaluate_predictions(\"ml_out_ds1.csv\", data_loc(\"dataset1_truth.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
